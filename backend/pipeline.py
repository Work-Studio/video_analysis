"""åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®èª¿åœãƒ­ã‚¸ãƒƒã‚¯."""

import json
from collections import Counter
from pathlib import Path
from typing import Any, Dict, List, Optional

import aiofiles

from backend.models.gemini_client import GeminiClient
from backend.models.risk_assessor import RiskAssessor
from backend.store import (
    PROJECT_STEPS,
    PipelineAlreadyRunningError,
    ProjectNotFoundError,
    ProjectStore,
)
from backend.utils.logging_utils import setup_logger


class AnalysisPipeline:
    """å‹•ç”»åˆ†æã®å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’é †æ¬¡å®Ÿè¡Œã™ã‚‹."""

    def __init__(
        self,
        *,
        store: ProjectStore,
        gemini_client: GeminiClient,
        risk_assessor: RiskAssessor,
        logger_name: str = "analysis_pipeline",
    ) -> None:
        self.store = store
        self.gemini_client = gemini_client
        self.risk_assessor = risk_assessor
        self.logger = setup_logger(logger_name)

    async def run(self, project_id: str) -> None:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ."""

        total_iterations = 3

        try:
            await self.store.mark_pipeline_started(project_id)
        except PipelineAlreadyRunningError:
            # æ—¢ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°æ¸ˆã¿ã§ã‚ã‚Œã°ãã®ã¾ã¾ç¶™ç¶š
            self.logger.info("Pipeline already running for project %s", project_id)
        except ProjectNotFoundError:
            self.logger.warning("Project %s not found. Abort pipeline.", project_id)
            return

        try:
            project = await self.store.get_project(project_id)
            video_path = Path(project.video_path)
            workspace_dir = Path(project.workspace_dir)
            media_type = project.media_type

            await self.store.update_iteration_state(
                project_id,
                current_iteration=0,
                total_iterations=total_iterations,
            )

            # æƒ…å ±æ‘˜å‡ºãƒ•ã‚§ãƒ¼ã‚ºã‚’2å›å®Ÿè¡Œã—ã€é©åˆ‡ãªæ–¹ã‚’é¸æŠ
            self.logger.info("Starting information extraction (run 1/2) for project %s", project_id)
            transcript_1, transcript_path_1, transcript_source_1, transcript_note_1 = await self._run_transcription(
                project_id, video_path, workspace_dir, media_type
            )
            ocr_text_1, ocr_path_1, ocr_note_1 = await self._run_ocr(project_id, video_path, workspace_dir, media_type)
            video_result_1, video_path_result_1, video_note_1 = await self._run_visual_analysis(
                project_id, video_path, workspace_dir, media_type
            )
            self.logger.info("Information extraction run 1/2 completed for project %s", project_id)

            self.logger.info("Starting information extraction (run 2/2) for project %s", project_id)
            transcript_2, transcript_path_2, transcript_source_2, transcript_note_2 = await self._run_transcription(
                project_id, video_path, workspace_dir, media_type
            )
            ocr_text_2, ocr_path_2, ocr_note_2 = await self._run_ocr(project_id, video_path, workspace_dir, media_type)
            video_result_2, video_path_result_2, video_note_2 = await self._run_visual_analysis(
                project_id, video_path, workspace_dir, media_type
            )
            self.logger.info("Information extraction run 2/2 completed for project %s", project_id)

            # 2ã¤ã®çµæœã‚’æ¯”è¼ƒã—ã€ã‚ˆã‚Šé©åˆ‡ãªæ–¹ã‚’é¸æŠ
            self.logger.info("Selecting best extraction results for project %s", project_id)
            transcript, transcript_path, transcript_source, transcript_note = await self._select_best_transcription(
                (transcript_1, transcript_path_1, transcript_source_1, transcript_note_1),
                (transcript_2, transcript_path_2, transcript_source_2, transcript_note_2)
            )
            ocr_text, ocr_path, ocr_note = await self._select_best_ocr(
                (ocr_text_1, ocr_path_1, ocr_note_1),
                (ocr_text_2, ocr_path_2, ocr_note_2)
            )
            video_result, video_path_result, video_note = await self._select_best_video(
                (video_result_1, video_path_result_1, video_note_1),
                (video_result_2, video_path_result_2, video_note_2)
            )
            self.logger.info("Information extraction completed for project %s", project_id)

            # ãƒªã‚¹ã‚¯åˆ†æã‚’3å›å®Ÿè¡Œ
            risk_results: List[Dict[str, Any]] = []
            for iteration in range(1, total_iterations + 1):
                await self.store.update_iteration_state(
                    project_id,
                    current_iteration=iteration,
                    total_iterations=total_iterations,
                )
                self.logger.info(
                    "Starting risk analysis iteration %d/%d for project %s",
                    iteration,
                    total_iterations,
                    project_id,
                )
                risk_result, risk_path = await self._run_risk(
                    project_id,
                    transcript,
                    ocr_text,
                    video_result,
                    workspace_dir,
                )
                risk_results.append(risk_result)
                self.logger.info(
                    "Risk analysis iteration %d/%d completed for project %s",
                    iteration,
                    total_iterations,
                    project_id,
                )

            # ãƒªã‚¹ã‚¯åˆ†æçµæœã‚’çµ±åˆï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥ï¼‰
            aggregated_risk = self._aggregate_risk_results(risk_results)

            aggregation = await self._finalize_with_single_extraction(
                project_id,
                workspace_dir,
                media_type,
                transcript,
                transcript_source,
                transcript_note,
                transcript_path,
                ocr_text,
                ocr_note,
                ocr_path,
                video_result,
                video_note,
                video_path_result,
                aggregated_risk,
                risk_results,
            )
            await self._apply_step_overrides(project_id, aggregation["step_payloads"])
            await self.store.mark_pipeline_completed(project_id, aggregation["final_report"])
            self.logger.info(
                "Pipeline completed for project %s after %d iterations",
                project_id,
                total_iterations,
            )
        except Exception as exc:  # pylint: disable=broad-except
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ failed ã«ã—ã¦ãƒ­ã‚°ã‚’æ®‹ã™
            self.logger.exception("Pipeline execution failed for %s", project_id)
            await self.store.mark_pipeline_failed(project_id, str(exc))
            raise

    async def _execute_iteration(
        self,
        project_id: str,
        media_path: Path,
        workspace_dir: Path,
        media_type: str,
        iteration: int,
        total_iterations: int,
    ) -> Dict[str, Any]:
        """å˜ä¸€ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†ã®è§£æã‚’å®Ÿè¡Œã—ã€ä¸­é–“çµæœã‚’è¿”ã™."""

        transcript, _, transcript_source, transcript_note = await self._run_transcription(
            project_id, media_path, workspace_dir, media_type
        )
        ocr_text, _, ocr_note = await self._run_ocr(project_id, media_path, workspace_dir, media_type)
        video_result, _, video_note = await self._run_visual_analysis(
            project_id, media_path, workspace_dir, media_type
        )
        risk_result, _ = await self._run_risk(
            project_id,
            transcript,
            ocr_text,
            video_result,
            workspace_dir,
        )
        self.logger.info(
            "Iteration %d/%d finished for %s",
            iteration,
            total_iterations,
            project_id,
        )
        return {
            "iteration": iteration,
            "transcription": {
                "text": transcript,
                "source": transcript_source,
                "note": transcript_note,
            },
            "ocr": {
                "text": ocr_text,
                "note": ocr_note,
            },
            "video": {
                "result": video_result,
                "note": video_note,
            },
            "risk": {
                "result": risk_result,
            },
        }

    def _aggregate_risk_results(self, risk_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """3å›ã®ãƒªã‚¹ã‚¯åˆ†æçµæœã‚’ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥ã§çµ±åˆ."""

        # 1. ç¤¾ä¼šçš„ãƒªã‚¹ã‚¯ãƒ»æ³•çš„ãƒªã‚¹ã‚¯: æœ€ã‚‚å³ã—ã„è©•ä¾¡ã‚’æ¡ç”¨
        social_grades = [result.get("social", {}).get("grade") for result in risk_results if result.get("social")]
        legal_grades = [result.get("legal", {}).get("grade") for result in risk_results if result.get("legal")]

        # ã‚°ãƒ¬ãƒ¼ãƒ‰ã®å„ªå…ˆåº¦ï¼ˆå³ã—ã„é †ï¼‰
        social_priority = {"S": 0, "A": 1, "B": 2, "C": 3}
        legal_priority = {"æŠµè§¦ã™ã‚‹": 0, "æŠµè§¦ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹": 1, "æŠµè§¦ã—ãªã„": 2}

        most_severe_social = min(social_grades, key=lambda g: social_priority.get(g, 99), default="C") if social_grades else "C"
        most_severe_legal = min(legal_grades, key=lambda g: legal_priority.get(g, 99), default="æŠµè§¦ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹") if legal_grades else "æŠµè§¦ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹"

        # æœ€ã‚‚å³ã—ã„è©•ä¾¡ã‚’æŒã¤çµæœã‚’é¸æŠ
        selected_result = None
        for result in risk_results:
            if (result.get("social", {}).get("grade") == most_severe_social or
                result.get("legal", {}).get("grade") == most_severe_legal):
                selected_result = result
                break

        if not selected_result:
            selected_result = risk_results[0] if risk_results else {}

        # 2. ã‚¿ã‚°: 2å›ä»¥ä¸Šå‡ºç¾ã—ãŸã‚¿ã‚°ã®ã¿æ¡ç”¨ï¼ˆå¤šæ•°æ±ºï¼‰
        all_tags = []
        for result in risk_results:
            tags = result.get("tags") or []
            all_tags.extend(tags)

        tag_counter: Dict[str, List[Dict[str, Any]]] = {}
        for tag in all_tags:
            tag_name = tag.get("name")
            if tag_name:
                if tag_name not in tag_counter:
                    tag_counter[tag_name] = []
                tag_counter[tag_name].append(tag)

        consensus_tags = []
        for tag_name, tag_list in tag_counter.items():
            if len(tag_list) >= 2:  # 2å›ä»¥ä¸Šå‡ºç¾
                # æœ€ã‚‚å³ã—ã„ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’é¸æŠ
                grade_priority = {"S": 0, "A": 1, "B": 2, "C": 3}
                best_tag = min(tag_list, key=lambda t: grade_priority.get(t.get("grade", "C"), 99))
                consensus_tags.append(best_tag)

        # 3. æ¤œå‡ºæ–‡è¨€: å…¨ã¦ã®åˆ†æã‹ã‚‰åé›†ã—ã€é‡è¤‡é™¤å¤–
        detected_phrases_set = set()
        for result in risk_results:
            social_findings = result.get("social", {}).get("findings") or []
            legal_findings = result.get("legal", {}).get("findings") or []
            for finding in social_findings + legal_findings:
                detail = finding.get("detail")
                if detail:
                    detected_phrases_set.add(detail)

        # çµ±åˆçµæœã‚’æ§‹ç¯‰
        aggregated_risk = {
            "social": selected_result.get("social", {}),
            "legal": selected_result.get("legal", {}),
            "matrix": selected_result.get("matrix", {"x_axis": "", "y_axis": "", "position": [0, 0]}),
            "tags": consensus_tags,
        }

        # burn_riskã‚’å†è¨ˆç®—
        burn_risk = self.risk_assessor.calculate_burn_risk(consensus_tags)
        aggregated_risk["burn_risk"] = burn_risk

        return aggregated_risk

    async def _finalize_with_single_extraction(
        self,
        project_id: str,
        workspace_dir: Path,
        media_type: str,
        transcript: str,
        transcript_source: str,
        transcript_note: Optional[str],
        transcript_path: Path,
        ocr_text: str,
        ocr_note: Optional[str],
        ocr_path: Path,
        video_result: Dict[str, Any],
        video_note: Optional[str],
        video_path: Path,
        aggregated_risk: Dict[str, Any],
        risk_results: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """æƒ…å ±æ‘˜å‡º1å›+ãƒªã‚¹ã‚¯åˆ†æ3å›ã®çµæœã‹ã‚‰æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã¨ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ."""

        transcript_formatted = self._format_transcript(transcript)
        ocr_formatted = self._format_ocr_text(ocr_text)
        video_formatted = self._format_video_analysis(video_result)
        risk_formatted = self._format_risk(aggregated_risk)

        # ãƒªã‚¹ã‚¯è©•ä¾¡çµæœã‚’ä¿å­˜
        risk_path = await self._save_json_file(
            workspace_dir,
            "risk_assessment.json",
            aggregated_risk,
        )

        # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
        iterations_serialized = [
            {
                "index": i + 1,
                "transcription": transcript,
                "ocr": ocr_text,
                "video_analysis": video_result,
                "risk": risk_result,
            }
            for i, risk_result in enumerate(risk_results)
        ]

        final_report = self._build_final_report(
            transcript,
            ocr_text,
            video_result,
            transcript_path,
            ocr_path,
            video_path,
            risk_path,
            aggregated_risk,
            transcript_source,
            transcript_note,
            ocr_note,
            video_note,
            iterations=iterations_serialized,
        )

        step_payloads: Dict[str, Dict[str, Any]] = {
            PROJECT_STEPS[0]: {
                "preview": transcript_formatted,
                "data": {
                    "transcript": transcript,
                    "formatted": transcript_formatted,
                    "file_path": str(transcript_path),
                    "source": transcript_source,
                    "note": transcript_note,
                },
            },
            PROJECT_STEPS[1]: {
                "preview": ocr_formatted,
                "data": {
                    "ocr_text": ocr_text,
                    "formatted": ocr_formatted,
                    "file_path": str(ocr_path),
                    "note": ocr_note,
                },
            },
            PROJECT_STEPS[2]: {
                "preview": video_formatted,
                "data": {
                    "raw": video_result,
                    "formatted": video_formatted,
                    "file_path": str(video_path),
                    "note": video_note,
                },
            },
            PROJECT_STEPS[3]: {
                "preview": risk_formatted,
                "data": {
                    "risk": aggregated_risk,
                    "formatted": risk_formatted,
                    "file_path": str(risk_path),
                    "runs": [
                        {
                            "iteration": i + 1,
                            "result": risk_result,
                        }
                        for i, risk_result in enumerate(risk_results)
                    ],
                },
            },
        }

        return {
            "final_report": final_report,
            "step_payloads": step_payloads,
        }

    async def _apply_step_overrides(
        self,
        project_id: str,
        step_payloads: Dict[str, Dict[str, Any]],
    ) -> None:
        """é›†ç´„å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã§ã‚¹ãƒˆã‚¢ã‚’æ›´æ–°ã™ã‚‹."""

        project = await self.store.get_project(project_id)
        for step, payload in step_payloads.items():
            preview = str(payload.get("preview") or "")
            project.payloads[step] = {
                "preview": preview[:300],
                "data": payload.get("data"),
            }
            project.step_status[step] = "completed"
        await self.store.save(project)

    async def _run_transcription(
        self, project_id: str, media_path: Path, workspace_dir: Path, media_type: str
    ) -> tuple[str, Path, str, Optional[str]]:
        """éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚¹ãƒ†ãƒƒãƒ—."""

        step = PROJECT_STEPS[0]
        await self.store.mark_step_running(project_id, step)
        transcript_source = "gemini"
        transcript_note: Optional[str] = None
        if media_type == "image":
            self.logger.info("Skipping transcription for %s (image asset).", project_id)
            transcript = ""
            transcript_source = "skipped"
            transcript_note = "é™æ­¢ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãŸã‚éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚"
            formatted = "ğŸ—£ï¸ éŸ³å£°æ–‡å­—èµ·ã“ã—\né™æ­¢ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãŸã‚éŸ³å£°æ–‡å­—èµ·ã“ã—ã¯å®Ÿæ–½ã—ã¾ã›ã‚“ã€‚"
            transcript_path = await self._save_text_file(
                workspace_dir,
                "transcription.txt",
                "é™æ­¢ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãŸã‚éŸ³å£°æ–‡å­—èµ·ã“ã—ã¯å®Ÿæ–½ã—ã¾ã›ã‚“ã€‚",
            )
        else:
            try:
                transcript = await self.gemini_client.run_step(
                    "transcription",
                    media_path,
                    media_type=media_type,
                )
            except Exception as gemini_error:
                self.logger.warning(
                    "Gemini transcription failed for %s: %s",
                    project_id,
                    gemini_error,
                )
                transcript = (
                    "æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸã€‚éŸ³å£°ãŒç¢ºèªã§ããªã„ãŸã‚ã€"
                    "å†åº¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚„åˆ¥ãƒ¢ãƒ‡ãƒ«ã§ã®è§£æã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
                )
                transcript_source = "fallback"
                transcript_note = (
                    "Gemini ã§ã®æ–‡å­—èµ·ã“ã—ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ–‡ç« ã‚’è¿”å´ã—ã¾ã—ãŸã€‚"
                )
            formatted = self._format_transcript(transcript)
            transcript_path = await self._save_text_file(
                workspace_dir, "transcription.txt", transcript or formatted
            )
        await self.store.update_status(
            project_id,
            step,
            formatted,
            data={
                "transcript": transcript,
                "formatted": formatted,
                "file_path": str(transcript_path),
                "source": transcript_source,
                "note": transcript_note,
            },
        )
        return transcript, transcript_path, transcript_source, transcript_note

    async def _run_ocr(
        self, project_id: str, video_path: Path, workspace_dir: Path, media_type: str
    ) -> tuple[str, Path, Optional[str]]:
        """OCR ã‚¹ãƒ†ãƒƒãƒ—."""

        step = PROJECT_STEPS[1]
        await self.store.mark_step_running(project_id, step)
        ocr_note: Optional[str] = None
        try:
            ocr_text = await self.gemini_client.run_step(
                "ocr",
                video_path,
                media_type=media_type,
            )
        except Exception as exc:
            self.logger.warning(
                "Gemini OCR failed for %s: %s", project_id, exc
            )
            ocr_text = (
                "OCR æŠ½å‡ºã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸã€‚è©²å½“ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ–‡å­—ãŒå–å¾—ã§ããªã‹ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            )
            ocr_note = "Gemini OCR ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ–‡ç« ã‚’è¿”å´ã—ã¾ã—ãŸã€‚"
        annotations = [
            line.strip()
            for line in ocr_text.splitlines()
            if "â€»" in line
        ]
        formatted = self._format_ocr_text(ocr_text)
        ocr_path = await self._save_text_file(workspace_dir, "ocr.txt", ocr_text)
        await self.store.update_status(
            project_id,
            step,
            formatted,
            data={
                "ocr_text": ocr_text,
                "formatted": formatted,
                "file_path": str(ocr_path),
                "note": ocr_note,
                "annotations": annotations,
            },
        )
        return ocr_text, ocr_path, ocr_note

    async def _run_visual_analysis(
        self, project_id: str, media_path: Path, workspace_dir: Path, media_type: str
    ) -> tuple[dict, Path, Optional[str]]:
        """æ˜ åƒè§£æã‚¹ãƒ†ãƒƒãƒ—."""

        step = PROJECT_STEPS[2]
        await self.store.mark_step_running(project_id, step)
        video_note: Optional[str] = None
        try:
            video_result = await self.gemini_client.run_step(
                "visual",
                media_path,
                media_type=media_type,
            )
            if not isinstance(video_result, dict):
                raise ValueError("Visual analysis returned non-dict payload")
            if media_type == "image":
                video_note = "Gemini ã«ã‚ˆã‚‹é™æ­¢ç”»è§£æã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚"
        except Exception as visual_error:
            self.logger.warning(
                "Gemini visual analysis failed for %s: %s",
                project_id,
                visual_error,
            )
            video_result = {
                "summary": (
                    "æ˜ åƒè§£æã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸã€‚Gemini API ã‚­ãƒ¼ã®è¨­å®šã‚’ç¢ºèªã—å†åº¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
                ),
                "segments": [],
                "risk_flags": ["analysis-unavailable"],
            }
            video_note = (
                "Gemini ã®æ˜ åƒè§£æã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼çµæœã‚’è¿”å´ã—ã¾ã—ãŸã€‚"
            )
        formatted = self._format_video_analysis(video_result)
        if self._is_stub_video_result(video_result):
            if video_note is None:
                video_note = "Gemini API ã‚­ãƒ¼æœªè¨­å®šã®ãŸã‚ã‚¹ã‚¿ãƒ–è§£æçµæœã‚’è¿”å´ã—ã¾ã—ãŸã€‚"
            self.logger.info("Visual analysis returned stub result for %s.", project_id)
        video_path = await self._save_json_file(workspace_dir, "video_analysis.json", video_result)
        await self.store.update_status(
            project_id,
            step,
            formatted,
            data={
                "raw": video_result,
                "formatted": formatted,
                "file_path": str(video_path),
                "note": video_note,
            },
        )
        return video_result, video_path, video_note

    async def _run_risk(
        self,
        project_id: str,
        transcript: str,
        ocr_text: str,
        video_result: dict,
        workspace_dir: Path,
    ) -> tuple[dict, Path]:
        """Gemini ã‚’ç”¨ã„ãŸçµ±åˆãƒªã‚¹ã‚¯è©•ä¾¡."""

        step = PROJECT_STEPS[3]
        await self.store.mark_step_running(project_id, step)
        try:
            risk_result = await self.risk_assessor.assess_with_enrichment(
                transcript=transcript,
                ocr_text=ocr_text,
                video_summary=video_result,
            )
            risk_result.setdefault("tags", [])
            burn_risk = self.risk_assessor.calculate_burn_risk(risk_result.get("tags") or [])
            risk_result["burn_risk"] = burn_risk
            self.logger.info(
                "Risk assessment finished for %s: social=%s legal=%s tags=%d burn_entries=%d",
                project_id,
                risk_result.get("social", {}).get("grade"),
                risk_result.get("legal", {}).get("grade"),
                len(risk_result.get("tags") or []),
                burn_risk.get("count") if isinstance(burn_risk, dict) else 0,
            )
        except Exception as exc:  # pragma: no cover
            self.logger.exception("Risk assessment failed for %s", project_id)
            risk_result = {
                "social": {
                    "grade": "C",
                    "reason": "ãƒªã‚¹ã‚¯è©•ä¾¡ã«å¤±æ•—ã—ãŸãŸã‚æš«å®šè©•ä¾¡ã‚’è¿”å´ã—ã¦ã„ã¾ã™ã€‚",
                    "findings": [],
                },
                "legal": {
                    "grade": "æŠµè§¦ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹",
                    "reason": "ãƒªã‚¹ã‚¯è©•ä¾¡ã«å¤±æ•—ã—ãŸãŸã‚æš«å®šè©•ä¾¡ã‚’è¿”å´ã—ã¦ã„ã¾ã™ã€‚",
                    "recommendations": "Gemini ã®è¨­å®šã‚’ç¢ºèªã—ã€å†åº¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚",
                    "violations": [],
                    "findings": [],
                },
                "matrix": {"x_axis": "æ³•å‹™è©•ä¾¡", "y_axis": "ç¤¾ä¼šçš„æ„Ÿåº¦", "position": [1, 2]},
                "note": str(exc),
                "tags": [],
                "burn_risk": {"count": 0, "details": []},
            }
        formatted = self._format_risk(risk_result)
        risk_path = await self._save_json_file(workspace_dir, "risk_assessment.json", risk_result)
        await self.store.update_status(
            project_id,
            step,
            formatted,
            data={
                "risk": risk_result,
                "formatted": formatted,
                "file_path": str(risk_path),
            },
        )
        return risk_result, risk_path

    def _build_final_report(
        self,
        transcript: str,
        ocr_text: str,
        video_result: dict,
        transcript_path: Path,
        ocr_path: Path,
        video_path: Path,
        risk_path: Path,
        risk_result: dict,
        transcription_source: str,
        transcription_note: Optional[str],
        ocr_note: Optional[str],
        video_note: Optional[str],
        iterations: Optional[List[Dict[str, Any]]] = None,
    ) -> dict:
        """å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®çµæœã‚’äººãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã§ã¾ã¨ã‚ã‚‹."""

        transcript_section = self._format_transcript(transcript)
        ocr_section = self._format_ocr_text(ocr_text)
        video_section = self._format_video_analysis(video_result)

        ocr_annotations = [
            line.strip()
            for line in ocr_text.splitlines()
            if "â€»" in line
        ]

        burn_risk = risk_result.get("burn_risk") if isinstance(risk_result, dict) else None

        social_grade = risk_result.get("social", {}).get("grade", "N/A")
        legal_grade = risk_result.get("legal", {}).get("grade", "N/A")

        disclaimer = (
            "*æœ¬åˆ†æçµæœã¯å‚è€ƒç”¨é€”ã®ã¿ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€ç¤¾ä¼šçš„ãƒ»æ³•çš„ãƒªã‚¹ã‚¯ã®ä¸å­˜åœ¨ã‚’ä¿è¨¼ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        )

        metadata: dict[str, object] = {
            "transcription_source": transcription_source,
        }
        if transcription_note:
            metadata["transcription_note"] = transcription_note
        if ocr_note:
            metadata["ocr_note"] = ocr_note
        if video_note:
            metadata["video_note"] = video_note
        if ocr_annotations:
            metadata["ocr_annotations"] = ocr_annotations
        if burn_risk:
            metadata["burn_risk"] = burn_risk

        return {
            "summary": disclaimer,
            "sections": {
                "transcription": transcript_section,
                "ocr": ocr_section,
                "video_analysis": video_section,
            },
            "files": {
                "transcription": str(transcript_path),
                "ocr": str(ocr_path),
                "video_analysis": str(video_path),
                "risk_assessment": str(risk_path),
            },
            "metadata": metadata,
            "risk": risk_result,
            "iterations": iterations,
        }

    def _format_transcript(self, transcript: str) -> str:
        excerpt = transcript.strip() or "éŸ³å£°ã‹ã‚‰æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆã¯å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        return f"ğŸ—£ï¸ éŸ³å£°æ–‡å­—èµ·ã“ã—\n{excerpt}"

    def _format_ocr_text(self, ocr_text: str) -> str:
        excerpt = ocr_text.strip() or "å­—å¹•æƒ…å ±ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
        return f"ğŸ“ OCRå­—å¹•æŠœç²‹\n{excerpt}"

    def _format_video_analysis(self, video_result: dict) -> str:
        summary = video_result.get("summary") or "æ˜ åƒã«é–¢ã™ã‚‹ç‰¹è¨˜äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        segments = video_result.get("segments") or []
        lines = [f"ğŸ¬ æ˜ åƒè§£æãƒ¬ãƒãƒ¼ãƒˆ\n{summary}"]
        if segments:
            lines.append("\nğŸ“‹ è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—")
            for segment in segments:
                label = segment.get("label", "æœªåˆ†é¡ã®è¡¨ç¾")
                description = segment.get("description", "")
                lines.append(f"- {label}")
                if description:
                    lines.append(f"  ãƒ»{description}")
                shots = segment.get("shots") or []
                for shot in shots:
                    timecode = shot.get("timecode", "timecodeä¸æ˜")
                    detail = shot.get("description", "")
                    lines.append(f"    - {timecode}: {detail}")
        risk_flags = video_result.get("risk_flags") or []
        if risk_flags:
            lines.append("\nâš ï¸ æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ")
            for flag in risk_flags:
                lines.append(f"- {flag}")
        return "\n".join(lines)

    def _format_risk(self, risk_result: dict) -> str:
        social = risk_result.get("social", {})
        legal = risk_result.get("legal", {})
        matrix = risk_result.get("matrix", {})
        lines = [
            "âš–ï¸ çµ±åˆãƒªã‚¹ã‚¯è©•ä¾¡",
            f"ç¤¾ä¼šçš„æ„Ÿåº¦: {social.get('grade', 'N/A')} - {social.get('reason', '')}",
            f"æ³•å‹™è©•ä¾¡: {legal.get('grade', 'N/A')} - {legal.get('reason', '')}",
        ]
        social_findings = social.get("findings") or []
        if social_findings:
            lines.append("  ãƒ»ç¤¾ä¼šçš„æ„Ÿåº¦æŒ‡æ‘˜:")
            for finding in social_findings[:5]:
                lines.append(
                    f"    - {finding.get('timecode', 'N/A')}: {finding.get('detail', '')}"
                )
        recommendations = legal.get("recommendations")
        if recommendations:
            lines.append(f"æ”¹å–„ææ¡ˆ: {recommendations}")
        legal_findings = legal.get("findings") or []
        if legal_findings:
            lines.append("  ãƒ»æ³•å‹™æŒ‡æ‘˜:")
            for finding in legal_findings[:5]:
                lines.append(
                    f"    - {finding.get('timecode', 'N/A')}: {finding.get('detail', '')}"
                )
        violations = legal.get("violations") or []
        if violations:
            lines.append("  ãƒ»æƒ³å®šã•ã‚Œã‚‹æŠµè§¦è¡¨ç¾:")
            for violation in violations[:5]:
                reference = violation.get("reference")
                expression = violation.get("expression", "")
                severity = violation.get("severity")
                detail = expression
                if severity:
                    detail = f"[{severity}] {detail}"
                if reference:
                    detail = f"{reference}: {detail}"
                lines.append(f"    - {detail}")
        burn_risk = risk_result.get("burn_risk") or {}
        if burn_risk.get("count"):
            lines.append(
                f"ç‚ä¸Šè£œæ­£: {burn_risk.get('grade', 'N/A')} ({burn_risk.get('label', '')}) å¹³å‡ãƒªã‚¹ã‚¯ {burn_risk.get('average', 'N/A')}"
            )
        position = matrix.get("position")
        if position:
            lines.append(f"ãƒã‚¸ã‚·ãƒ§ãƒ³: X={position[0]} / Y={position[1]}")
        tags = risk_result.get("tags") or []
        if tags:
            lines.append("\nğŸ§© ã‚¿ã‚°åˆ¥è©•ä¾¡")
            for tag in tags[:5]:
                lines.append(
                    f"- {tag.get('name', 'ä¸æ˜')}: {tag.get('grade', 'N/A')} / {tag.get('reason', '')}"
                )
        return "\n".join(lines)

    @staticmethod
    def _serialize_iterations(iteration_runs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        serialized: List[Dict[str, Any]] = []
        for run in iteration_runs:
            serialized.append(
                {
                    "index": run.get("iteration"),
                    "transcription": run.get("transcription", {}).get("text"),
                    "ocr": run.get("ocr", {}).get("text"),
                    "video_analysis": run.get("video", {}).get("result"),
                    "risk": run.get("risk", {}).get("result"),
                }
            )
        return serialized

    @staticmethod
    def _select_consensus_text(candidates: List[str]) -> str:
        cleaned = [text.strip() for text in candidates if text and text.strip()]
        if not cleaned:
            return candidates[-1] if candidates else ""
        counts = Counter(cleaned)
        top_text, top_count = counts.most_common(1)[0]
        if top_count == 1:
            return max(cleaned, key=len, default="")
        return top_text

    @staticmethod
    def _select_most_common_value(values: List[Optional[str]], default: str) -> str:
        filtered = [value for value in values if value]
        if not filtered:
            return default
        counts = Counter(filtered)
        return counts.most_common(1)[0][0]

    @staticmethod
    def _first_non_empty(values: List[Optional[str]]) -> Optional[str]:
        for value in values:
            if value:
                stripped = value.strip()
                if stripped:
                    return stripped
        return None

    @staticmethod
    def _select_video_payload(
        video_runs: List[Dict[str, Any]]
    ) -> tuple[Dict[str, Any], Optional[str]]:
        if not video_runs:
            return {}, None
        best_result: Dict[str, Any] = {}
        best_note: Optional[str] = None
        best_score = -1
        for run in video_runs:
            result = run.get("result") or {}
            segments = result.get("segments") or []
            score = len(segments)
            if score > best_score:
                best_score = score
                best_result = result
                best_note = run.get("note")
        return best_result, best_note

    @staticmethod
    def _is_stub_video_result(result: dict) -> bool:
        summary = result.get("summary", "")
        risk_flags = result.get("risk_flags") or []
        if isinstance(summary, str) and summary.startswith("[stub]"):
            return True
        return any(
            flag in {"insight-unavailable", "analysis-unavailable"} for flag in risk_flags
        )

    async def _save_text_file(self, workspace_dir: Path, filename: str, content: str) -> Path:
        """ãƒ†ã‚­ã‚¹ãƒˆçµæœã‚’ uploads ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜."""

        output_path = workspace_dir / filename
        async with aiofiles.open(output_path, "w", encoding="utf-8") as file_obj:
            await file_obj.write(content)
        return output_path

    async def _save_json_file(self, workspace_dir: Path, filename: str, payload: dict) -> Path:
        """JSON çµæœã‚’ uploads ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜."""

        output_path = workspace_dir / filename
        async with aiofiles.open(output_path, "w", encoding="utf-8") as file_obj:
            json_payload = json.dumps(payload, ensure_ascii=False, indent=2)
            await file_obj.write(json_payload)
        return output_path

    async def _select_best_transcription(
        self,
        run1: tuple[str, Path, str, Optional[str]],
        run2: tuple[str, Path, str, Optional[str]]
    ) -> tuple[str, Path, str, Optional[str]]:
        """2ã¤ã®æ–‡å­—èµ·ã“ã—çµæœã‚’æ¯”è¼ƒã—ã€ã‚ˆã‚Šé©åˆ‡ãªæ–¹ã‚’é¸æŠ."""
        transcript_1, path_1, source_1, note_1 = run1
        transcript_2, path_2, source_2, note_2 = run2

        # é•·ã•ã®æ¯”è¼ƒï¼ˆã‚ˆã‚Šè©³ç´°ãªæ–¹ã‚’å„ªå…ˆï¼‰
        len1 = len(transcript_1.strip())
        len2 = len(transcript_2.strip())

        # å·®ç•°ãŒ5%æœªæº€ãªã‚‰1ã¤ç›®ã‚’æ¡ç”¨ï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰
        if abs(len1 - len2) / max(len1, len2, 1) < 0.05:
            self.logger.info(f"Transcriptions are similar (diff < 5%), selecting run 1")
            return run1

        # Geminiã«åˆ¤å®šã‚’ä¾é ¼
        from textwrap import dedent
        prompt = dedent(f"""
        ä»¥ä¸‹ã®2ã¤ã®æ–‡å­—èµ·ã“ã—çµæœã‚’æ¯”è¼ƒã—ã€ã‚ˆã‚Šæ–‡è„ˆçš„ã«é©åˆ‡ã§å®Œå…¨ãªæ–¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

        ## æ–‡å­—èµ·ã“ã—çµæœ1 (é•·ã•: {len1}æ–‡å­—)
        {transcript_1[:3000]}

        ## æ–‡å­—èµ·ã“ã—çµæœ2 (é•·ã•: {len2}æ–‡å­—)
        {transcript_2[:3000]}

        ã©ã¡ã‚‰ãŒã‚ˆã‚Šé©åˆ‡ã‹ã€"1" ã¾ãŸã¯ "2" ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        ç†ç”±ã‚’ç°¡æ½”ã«èª¬æ˜ã—ãŸå¾Œã€æœ€å¾Œã®è¡Œã«æ•°å­—ã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
        """)

        try:
            response = await self.gemini_client.generate_text(prompt)
            choice_text = response.strip().split("\n")[-1].strip()
            choice = int(choice_text)
            self.logger.info(f"Gemini selected transcription {choice}")
            return run1 if choice == 1 else run2
        except Exception as e:
            self.logger.warning(f"Failed to get Gemini selection: {e}, defaulting to longer transcription")
            return run1 if len1 >= len2 else run2

    async def _select_best_ocr(
        self,
        run1: tuple[str, Path, Optional[str]],
        run2: tuple[str, Path, Optional[str]]
    ) -> tuple[str, Path, Optional[str]]:
        """2ã¤ã®OCRçµæœã‚’æ¯”è¼ƒã—ã€ã‚ˆã‚Šé©åˆ‡ãªæ–¹ã‚’é¸æŠ."""
        ocr_1, path_1, note_1 = run1
        ocr_2, path_2, note_2 = run2

        len1 = len(ocr_1.strip())
        len2 = len(ocr_2.strip())

        if abs(len1 - len2) / max(len1, len2, 1) < 0.05:
            self.logger.info(f"OCR results are similar (diff < 5%), selecting run 1")
            return run1

        from textwrap import dedent
        prompt = dedent(f"""
        ä»¥ä¸‹ã®2ã¤ã®OCRå­—å¹•æŠ½å‡ºçµæœã‚’æ¯”è¼ƒã—ã€ã‚ˆã‚Šæ–‡è„ˆçš„ã«é©åˆ‡ã§å®Œå…¨ãªæ–¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

        ## OCRçµæœ1 (é•·ã•: {len1}æ–‡å­—)
        {ocr_1[:3000]}

        ## OCRçµæœ2 (é•·ã•: {len2}æ–‡å­—)
        {ocr_2[:3000]}

        ã©ã¡ã‚‰ãŒã‚ˆã‚Šé©åˆ‡ã‹ã€"1" ã¾ãŸã¯ "2" ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        ç†ç”±ã‚’ç°¡æ½”ã«èª¬æ˜ã—ãŸå¾Œã€æœ€å¾Œã®è¡Œã«æ•°å­—ã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
        """)

        try:
            response = await self.gemini_client.generate_text(prompt)
            choice_text = response.strip().split("\n")[-1].strip()
            choice = int(choice_text)
            self.logger.info(f"Gemini selected OCR {choice}")
            return run1 if choice == 1 else run2
        except Exception as e:
            self.logger.warning(f"Failed to get Gemini selection: {e}, defaulting to longer OCR")
            return run1 if len1 >= len2 else run2

    async def _select_best_video(
        self,
        run1: tuple[Dict[str, Any], Path, Optional[str]],
        run2: tuple[Dict[str, Any], Path, Optional[str]]
    ) -> tuple[Dict[str, Any], Path, Optional[str]]:
        """2ã¤ã®æ˜ åƒè§£æçµæœã‚’æ¯”è¼ƒã—ã€ã‚ˆã‚Šé©åˆ‡ãªæ–¹ã‚’é¸æŠ."""
        video_1, path_1, note_1 = run1
        video_2, path_2, note_2 = run2

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ã¨ãƒ†ã‚­ã‚¹ãƒˆé‡ã§æ¯”è¼ƒ
        segments_1 = video_1.get("segments", [])
        segments_2 = video_2.get("segments", [])

        text_1 = " ".join([seg.get("description", "") for seg in segments_1])
        text_2 = " ".join([seg.get("description", "") for seg in segments_2])

        len1 = len(text_1.strip())
        len2 = len(text_2.strip())

        if abs(len1 - len2) / max(len1, len2, 1) < 0.05:
            self.logger.info(f"Video analysis results are similar (diff < 5%), selecting run 1")
            return run1

        from textwrap import dedent
        prompt = dedent(f"""
        ä»¥ä¸‹ã®2ã¤ã®æ˜ åƒè§£æçµæœã‚’æ¯”è¼ƒã—ã€ã‚ˆã‚Šæ–‡è„ˆçš„ã«é©åˆ‡ã§è©³ç´°ãªæ–¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

        ## æ˜ åƒè§£æ1 (ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(segments_1)}, èª¬æ˜æ–‡å­—æ•°: {len1})
        {json.dumps(video_1, ensure_ascii=False, indent=2)[:3000]}

        ## æ˜ åƒè§£æ2 (ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(segments_2)}, èª¬æ˜æ–‡å­—æ•°: {len2})
        {json.dumps(video_2, ensure_ascii=False, indent=2)[:3000]}

        ã©ã¡ã‚‰ãŒã‚ˆã‚Šé©åˆ‡ã‹ã€"1" ã¾ãŸã¯ "2" ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        ç†ç”±ã‚’ç°¡æ½”ã«èª¬æ˜ã—ãŸå¾Œã€æœ€å¾Œã®è¡Œã«æ•°å­—ã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
        """)

        try:
            response = await self.gemini_client.generate_text(prompt)
            choice_text = response.strip().split("\n")[-1].strip()
            choice = int(choice_text)
            self.logger.info(f"Gemini selected video analysis {choice}")
            return run1 if choice == 1 else run2
        except Exception as e:
            self.logger.warning(f"Failed to get Gemini selection: {e}, defaulting to more detailed video analysis")
            return run1 if len1 >= len2 else run2
